{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f904db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e520ee",
   "metadata": {},
   "source": [
    "## Question 1: Top 10 Bundesliga teams over entire period\n",
    "\n",
    "**Metric: Points Per Game (PPG)**\n",
    "- 3 points for win\n",
    "- 1 point for draw\n",
    "- 0 points for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/bundesliga.csv')\n",
    "\n",
    "# Convert Date to datetime \n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed', dayfirst=True)\n",
    "\n",
    "# Quick look\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDate range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5318d90",
   "metadata": {},
   "source": [
    "## Calculate Points for Each Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_team_stats(df):\n",
    "    stats = {}\n",
    "    \n",
    "    # Process matches\n",
    "    for _, row in df.iterrows():\n",
    "        home_team = row['HomeTeam']\n",
    "        away_team = row['AwayTeam']\n",
    "        result = row['FTR']\n",
    "        \n",
    "        # Initialize teams if not seen before\n",
    "        if home_team not in stats:\n",
    "            stats[home_team] = {'games': 0, 'points': 0}\n",
    "        if away_team not in stats:\n",
    "            stats[away_team] = {'games': 0, 'points': 0}\n",
    "        \n",
    "        # Update home team stats\n",
    "        stats[home_team]['games'] += 1\n",
    "        \n",
    "        if result == 'H': \n",
    "            stats[home_team]['points'] += 3\n",
    "        elif result == 'D':     \n",
    "            stats[home_team]['points'] += 1\n",
    "        \n",
    "        # Update away team stats\n",
    "        stats[away_team]['games'] += 1\n",
    "        \n",
    "        if result == 'A': \n",
    "            stats[away_team]['points'] += 3\n",
    "        elif result == 'D':     \n",
    "            stats[away_team]['points'] += 1\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267529c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stats\n",
    "team_stats = calculate_team_stats(df)\n",
    "\n",
    "# Convert to df\n",
    "stats_df = pd.DataFrame.from_dict(team_stats, orient='index')\n",
    "stats_df.index.name = 'Team'\n",
    "stats_df.reset_index(inplace=True)\n",
    "\n",
    "# Calculate derived metrics\n",
    "stats_df['PPG'] = stats_df['points'] / stats_df['games']\n",
    "\n",
    "print(f\"Total teams in dataset: {len(stats_df)}\")\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter teams with min 100 games and get top 10\n",
    "min_games = 100\n",
    "\n",
    "filtered_stats = stats_df[stats_df['games'] >= min_games]\n",
    "top_10 = filtered_stats.sort_values('PPG', ascending=False).head(10)\n",
    "\n",
    "print(f\"=== TOP 10 BUNDESLIGA TEAMS (1993-2018) ===\")\n",
    "print(f\"Filtered by minimum {min_games} games | Teams remaining: {len(filtered_stats)}\\n\")\n",
    "\n",
    "# Reset index to start from 1\n",
    "top_10_display = top_10[['Team', 'games', 'PPG']].reset_index(drop=True)\n",
    "top_10_display.index = range(1, 11)\n",
    "top_10_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of top 10 teams\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# PPG comparison\n",
    "ax.barh(range(len(top_10)), top_10['PPG'].values, color='steelblue')\n",
    "ax.set_yticks(range(len(top_10)))\n",
    "ax.set_yticklabels(top_10['Team'].values)\n",
    "ax.set_xlabel('Points Per Game', fontsize=12)\n",
    "ax.set_title('Top 10 Bundesliga Teams by PPG (1993-2018)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_10['PPG'].values):\n",
    "    ax.text(v + 0.02, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e7a88",
   "metadata": {},
   "source": [
    "### Summary - Question 1\n",
    "\n",
    "**Approach:**\n",
    "The chosen metric is points per game (PPG) using FIFA's 3-1-0 point system (3 for win, 1 for draw, 0 for loss) mirroring reality and preventing skews due to draws. A filter of minimum 100 games is placed to ensure statistical significance and exclude short term performance as 2 teams initially placed in the top 10 without this constraint. The complete dataset is covered from 1993 to 2018.\n",
    "\n",
    "**Key Findings:**\n",
    "Bayern Munich dominates with 2.141 PPG, significantly ahead of other teams. Other established Bundesliga teams cluster in the 1.5-1.7 range, while the bottom tier consolidates around 1.4. The above horizontal bar chart shows this hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c1d5a",
   "metadata": {},
   "source": [
    "## Question 2: Total goals prediction\n",
    "\n",
    "A model is implemented to predict total goals in football matches. Since bets are placed at half time the target variable for prediction is the total goals in the second half of the matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ba950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing halftime data (1993/94 and 1994/95 seasons)\n",
    "df_clean = df.dropna(subset=['HTHG', 'HTAG', 'HTR']).copy()\n",
    "\n",
    "print(f\"\\nAfter dropping missing halftime data: {df_clean.shape}\")\n",
    "print(f\"Rows dropped: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable: Total goals in second half\n",
    "df_clean['second_half_goals'] = (df_clean['FTHG'] - df_clean['HTHG']) + (df_clean['FTAG'] - df_clean['HTAG'])\n",
    "\n",
    "# Halftime features\n",
    "df_clean['ht_total_goals'] = df_clean['HTHG'] + df_clean['HTAG']\n",
    "df_clean['ht_goal_diff'] = df_clean['HTHG'] - df_clean['HTAG']\n",
    "\n",
    "df_clean['year'] = df_clean['Date'].dt.year\n",
    "\n",
    "# Calculate running average of second half goals for each team.\n",
    "def calculate_historical_avg(df, team_col):\n",
    "    is_home = team_col == 'HomeTeam'\n",
    "    team_avgs = {}\n",
    "    result = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        team = row[team_col]\n",
    "        \n",
    "        # Get average from prior matches only\n",
    "        if team in team_avgs and team_avgs[team]['games'] > 0:\n",
    "            avg = team_avgs[team]['sh_cumulative_goals'] / team_avgs[team]['games']\n",
    "        else:\n",
    "            avg = np.nan\n",
    "        \n",
    "        result.append(avg)\n",
    "        \n",
    "        # Update running totals after recording the average\n",
    "        if team not in team_avgs:\n",
    "            team_avgs[team] = {'sh_cumulative_goals': 0, 'games': 0}\n",
    "        \n",
    "        sh_goals = row['FTHG'] - row['HTHG'] if is_home else row['FTAG'] - row['HTAG']\n",
    "        team_avgs[team]['sh_cumulative_goals'] += sh_goals\n",
    "        team_avgs[team]['games'] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Calculate historical averages\n",
    "df_clean['home_team_hist_sh_goals'] = calculate_historical_avg(df_clean, 'HomeTeam')\n",
    "df_clean['away_team_hist_sh_goals'] = calculate_historical_avg(df_clean, 'AwayTeam')\n",
    "\n",
    "# Fill NaNs: avg. about 0.8 goals per team per second half\n",
    "AVG_SH_GOALS_PER_TEAM = 0.8\n",
    "\n",
    "df_clean['home_team_hist_sh_goals'] = df_clean['home_team_hist_sh_goals'].fillna(AVG_SH_GOALS_PER_TEAM)\n",
    "df_clean['away_team_hist_sh_goals'] = df_clean['away_team_hist_sh_goals'].fillna(AVG_SH_GOALS_PER_TEAM)\n",
    "\n",
    "df_clean['combined_hist_sh_goals'] = df_clean['home_team_hist_sh_goals'] + df_clean['away_team_hist_sh_goals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: Train on data before 2016, test on 2016-2018\n",
    "train_cutoff_year = 2016\n",
    "train_df = df_clean[df_clean['year'] < train_cutoff_year].copy()\n",
    "test_df = df_clean[df_clean['year'] >= train_cutoff_year].copy()\n",
    "\n",
    "print(f\"\\n=== Train-Test Split ===\")\n",
    "print(f\"Train set: {len(train_df)} matches ({train_df['year'].min()}-{train_df['year'].max()})\")\n",
    "print(f\"Test set: {len(test_df)} matches ({test_df['year'].min()}-{test_df['year'].max()})\")\n",
    "\n",
    "# Features\n",
    "feature_cols = [\n",
    "    'ht_total_goals', 'ht_goal_diff',  # Half time features\n",
    "    'home_team_hist_sh_goals',         # Historical: home team's avg 2nd half goals\n",
    "    'away_team_hist_sh_goals',         # Historical: away team's avg 2nd half goals\n",
    "]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['second_half_goals']\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['second_half_goals']\n",
    "\n",
    "print(f\"\\nFeatures used: {len(feature_cols)}\")\n",
    "for f in feature_cols:\n",
    "    print(f\"  - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ace620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: Predict mean of training \n",
    "baseline_pred = np.full(len(y_test), y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "\n",
    "print(f\"\\n=== Baseline (Predict Mean) ===\")\n",
    "print(f\"Mean second half goals (train): {y_train.mean():.3f}\")\n",
    "print(f\"MAE: {baseline_mae:.3f}\")\n",
    "print(f\"RMSE: {baseline_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "print(f\"\\nTraining Random Forest\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_pred_train = rf_model.predict(X_train)\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "rf_mae_train = mean_absolute_error(y_train, rf_pred_train)\n",
    "rf_rmse_train = np.sqrt(mean_squared_error(y_train, rf_pred_train))\n",
    "rf_r2_train = r2_score(y_train, rf_pred_train)\n",
    "\n",
    "rf_mae_test = mean_absolute_error(y_test, rf_pred_test)\n",
    "rf_rmse_test = np.sqrt(mean_squared_error(y_test, rf_pred_test))\n",
    "rf_r2_test = r2_score(y_test, rf_pred_test)\n",
    "\n",
    "print(f\"\\nTrain Performance:\")\n",
    "print(f\"  MAE: {rf_mae_train:.3f}\")\n",
    "print(f\"  RMSE: {rf_rmse_train:.3f}\")\n",
    "print(f\"  R²: {rf_r2_train:.3f}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  MAE: {rf_mae_test:.3f}\")\n",
    "print(f\"  RMSE: {rf_rmse_test:.3f}\")\n",
    "print(f\"  R²: {rf_r2_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparinng results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON (Test Set)\")\n",
    "print(\"=\"*50)\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline (Mean)', 'Random Forest'],\n",
    "    'MAE': [baseline_mae, rf_mae_test],\n",
    "    'RMSE': [baseline_rmse, rf_rmse_test],\n",
    "    'R²': [0.0, rf_r2_test]\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\nMAE improvement: {((baseline_mae - rf_mae_test) / baseline_mae * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(y_test, rf_pred_test, alpha=0.6, color='steelblue', s=50)\n",
    "\n",
    "min_val = 0\n",
    "max_val = max(y_test.max(), rf_pred_test.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=3, label='Perfect Prediction')\n",
    "\n",
    "ax.set_xlabel('Actual Second Half Goals', fontsize=12)\n",
    "ax.set_ylabel('Predicted Second Half Goals', fontsize=12)\n",
    "ax.set_title('Model Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax.set_xlim(-0.5, max_val + 0.5)\n",
    "ax.set_ylim(min_val - 0.2, max_val + 0.2)\n",
    "\n",
    "exact_matches = (np.round(rf_pred_test) == y_test).sum()\n",
    "total = len(y_test)\n",
    "ax.text(0.02, 0.98, f'Exact matches: {exact_matches}/{total} ({exact_matches/total*100:.1f}%)', \n",
    "        transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5a99d",
   "metadata": {},
   "source": [
    "### Summary - Question 2: Total Goals Prediction\n",
    "\n",
    "**Approach:**\n",
    "The target variable is second half goals (total goals - halftime goals) since bets are placed at halftime. Feature engineering settled on halftime context: total goals and goal difference at half time + historical features: Running averages of each team's second-half scoring patterns. The model chosen is a random forest regressor with conservative hyperparameters to avoid overfitting, although first thought was a poisson regressor. Evaluation is performed using a chronological time based split (train: pre-2016, test: 2016-2018) to simulate real world deployment.\n",
    "\n",
    "**Key findings:**\n",
    "Test MAE and R² shows tiny improvement over baseline and a 28.1% hitrate when rounded to nearest integer. Model consistently over-predicts low-scoring games and under-predicts high-scoring ones, essentially just conservatively guessing the mean for every outcome. On the above figure, points clustered in narrow horizontal band show model only predicts 1.2-2.6 goals while actual games range 0-6+ goals. \n",
    "**Potential improvements**\n",
    "The model has few features (4) as a result of poor performance with more features, both for poisson/random forest/XGBoost. Ideally more, and better features should improve model performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
